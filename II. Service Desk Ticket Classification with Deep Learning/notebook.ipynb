{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![servicedesk](image.png)\n",
    "\n",
    "CleverSupport is a company at the forefront of AI innovation, specializing in the development of AI-driven solutions to enhance customer support services. Their latest endeavor is to engineer a text classification system that can automatically categorize customer complaints. \n",
    "\n",
    "Our role as a data scientist involves the creation of a sophisticated machine learning model that can accurately assign complaints to specific categories, such as mortgage, credit card, money transfers, debt collection, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.4.0.post0)\n",
      "Collecting nltk (from -r requirements.txt (line 2))\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting pandas (from -r requirements.txt (line 3))\n",
      "  Using cached pandas-2.0.3-cp38-cp38-macosx_10_9_x86_64.whl.metadata (18 kB)\n",
      "Collecting scikit-learn (from -r requirements.txt (line 4))\n",
      "  Using cached scikit_learn-1.3.2-cp38-cp38-macosx_10_9_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torch in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (2.2.2)\n",
      "Requirement already satisfied: numpy>1.20.0 in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from torchmetrics->-r requirements.txt (line 1)) (1.24.4)\n",
      "Requirement already satisfied: packaging>17.1 in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from torchmetrics->-r requirements.txt (line 1)) (24.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from torchmetrics->-r requirements.txt (line 1)) (0.11.3.post0)\n",
      "Requirement already satisfied: typing-extensions in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from torchmetrics->-r requirements.txt (line 1)) (4.12.2)\n",
      "Collecting click (from nltk->-r requirements.txt (line 2))\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk->-r requirements.txt (line 2))\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk->-r requirements.txt (line 2))\n",
      "  Downloading regex-2024.5.15-cp38-cp38-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm (from nltk->-r requirements.txt (line 2))\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->-r requirements.txt (line 3))\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas->-r requirements.txt (line 3))\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting scipy>=1.5.0 (from scikit-learn->-r requirements.txt (line 4))\n",
      "  Using cached scipy-1.10.1-cp38-cp38-macosx_10_9_x86_64.whl.metadata (53 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->-r requirements.txt (line 4))\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from torch->-r requirements.txt (line 5)) (3.15.4)\n",
      "Requirement already satisfied: sympy in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from torch->-r requirements.txt (line 5)) (1.13.0)\n",
      "Requirement already satisfied: networkx in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from torch->-r requirements.txt (line 5)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from torch->-r requirements.txt (line 5)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from torch->-r requirements.txt (line 5)) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from lightning-utilities>=0.8.0->torchmetrics->-r requirements.txt (line 1)) (41.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from jinja2->torch->-r requirements.txt (line 5)) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from sympy->torch->-r requirements.txt (line 5)) (1.3.0)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached pandas-2.0.3-cp38-cp38-macosx_10_9_x86_64.whl (11.7 MB)\n",
      "Using cached scikit_learn-1.3.2-cp38-cp38-macosx_10_9_x86_64.whl (10.1 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Downloading regex-2024.5.15-cp38-cp38-macosx_10_9_x86_64.whl (281 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.8/281.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached scipy-1.10.1-cp38-cp38-macosx_10_9_x86_64.whl (35.0 MB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: pytz, tzdata, tqdm, threadpoolctl, scipy, regex, joblib, click, scikit-learn, pandas, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.8.1 pandas-2.0.3 pytz-2024.1 regex-2024.5.15 scikit-learn-1.3.2 scipy-1.10.1 threadpoolctl-3.5.0 tqdm-4.66.4 tzdata-2024.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchmetrics import Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1108)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imprt data and labels\n",
    "with open(\"data/words.json\", 'r') as f1:\n",
    "    words = json.load(f1)\n",
    "with open(\"data/text.json\", 'r') as f2:\n",
    "    text = json.load(f2)\n",
    "labels = np.load('data/labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries to store the word to index mappings and vice versa\n",
    "word2idx = {o:i for i, o in enumerate(words)}\n",
    "idx2word = {i:o for i, o in enumerate(words)}\n",
    "\n",
    "# Looking up the mapping dictionary and assigning the index to the respective words\n",
    "for i, sentece in enumerate(text):\n",
    "    text[i] = [word2idx[word] if word in word2idx else 0 for word in sentece]\n",
    "    \n",
    "# Defining a function that either shortens sentences or pads sentences with 0 ot a fixed length\n",
    "def pad_input(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len), dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) !=0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "        return features\n",
    "\n",
    "text = pad_input(text, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset\n",
    "train_text, test_text, train_label, test_label = train_test_split(text, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_text), torch.from_numpy(train_label).long())\n",
    "test_data = TensorDataset(torch.from_numpy(test_text), torch.from_numpy(test_label).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 400\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier class\n",
    "class TicketClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, target_size):\n",
    "        super(TicketClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(embed_dim, target_size)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text).permute(0, 2, 1)\n",
    "        conved = F.relu(self.conv(embedded))\n",
    "        conved = conved.mean(dim=2)\n",
    "        return self.fc(conved)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
