{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![clothing_classification](clothing_classification.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion Forward is a new AI-based e-commerce clothing retailer.\n",
    "They want to use image classification to automatically categorize new product listings, making it easier for customers to find what they're looking for. It will also assist in inventory management by quickly sorting items.\n",
    "\n",
    "As a data scientist tasked with implementing a garment classifier, your primary objective is to develop a machine learning model capable of accurately categorizing images of clothing items into distinct garment types such as shirts, trousers, shoes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>1.20.0 (from torchmetrics)\n",
      "  Using cached numpy-1.24.4-cp38-cp38-macosx_10_9_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: packaging>17.1 in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from torchmetrics) (24.1)\n",
      "Collecting torch>=1.10.0 (from torchmetrics)\n",
      "  Downloading torch-2.2.2-cp38-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Downloading lightning_utilities-0.11.3.post0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: typing-extensions in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from torchmetrics) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (41.2.0)\n",
      "Collecting filelock (from torch>=1.10.0->torchmetrics)\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting sympy (from torch>=1.10.0->torchmetrics)\n",
      "  Downloading sympy-1.13.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.10.0->torchmetrics)\n",
      "  Using cached networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting jinja2 (from torch>=1.10.0->torchmetrics)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch>=1.10.0->torchmetrics)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.10.0->torchmetrics)\n",
      "  Using cached MarkupSafe-2.1.5-cp38-cp38-macosx_10_9_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch>=1.10.0->torchmetrics)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.11.3.post0-py3-none-any.whl (26 kB)\n",
      "Using cached numpy-1.24.4-cp38-cp38-macosx_10_9_x86_64.whl (19.8 MB)\n",
      "Downloading torch-2.2.2-cp38-none-macosx_10_9_x86_64.whl (150.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.6/150.6 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Downloading sympy-1.13.0-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached MarkupSafe-2.1.5-cp38-cp38-macosx_10_9_x86_64.whl (14 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, numpy, networkx, MarkupSafe, lightning-utilities, fsspec, filelock, jinja2, torch, torchmetrics\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.15.4 fsspec-2024.6.1 jinja2-3.1.4 lightning-utilities-0.11.3.post0 mpmath-1.3.0 networkx-3.1 numpy-1.24.4 sympy-1.13.0 torch-2.2.2 torchmetrics-1.4.0.post0\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.17.2-cp38-cp38-macosx_10_13_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from torchvision) (1.24.4)\n",
      "Requirement already satisfied: torch==2.2.2 in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from torchvision) (2.2.2)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Downloading pillow-10.4.0-cp38-cp38-macosx_10_10_x86_64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: filelock in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from torch==2.2.2->torchvision) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from torch==2.2.2->torchvision) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from torch==2.2.2->torchvision) (1.13.0)\n",
      "Requirement already satisfied: networkx in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from torch==2.2.2->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from torch==2.2.2->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from torch==2.2.2->torchvision) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from jinja2->torch==2.2.2->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/sina/Documents/GitHub_Local/PyTorch_Lab/.venv/lib/python3.8/site-packages (from sympy->torch==2.2.2->torchvision) (1.3.0)\n",
      "Downloading torchvision-0.17.2-cp38-cp38-macosx_10_13_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.4.0-cp38-cp38-macosx_10_10_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pillow, torchvision\n",
      "Successfully installed pillow-10.4.0 torchvision-0.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of classes\n",
    "classes = train_data.classes\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some relevant variables\n",
    "num_input_channels = 1\n",
    "num_output_channels = 16\n",
    "image_size = train_data[0][0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN\n",
    "class MultiClassImageClassifier(nn.Module):\n",
    "    \n",
    "    # Define the inint method\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultiClassImageClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_input_channels, num_output_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpoll = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Create a fully connected layer\n",
    "        self.fc = nn.Linear(num_output_channels * (image_size//2)**2, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass inputs through each layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpoll(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training set DataLoader\n",
    "dataloader_train = DataLoader(\n",
    "    train_data, \n",
    "    batch_size=10, \n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define trianing function\n",
    "def train_model(optimizer, net, num_epochs):\n",
    "    num_processed = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        num_processed = 0\n",
    "        for features, labels in dataloader_train:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            num_processed += len(labels)\n",
    "        print(f'epoch {epoch}, loss: {running_loss / num_processed}')\n",
    "    \n",
    "    train_loss = running_loss / len(dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 0.04020518061496162\n"
     ]
    }
   ],
   "source": [
    "# Train for 1 epcho\n",
    "net = MultiClassImageClassifier(num_classes)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "train_model(\n",
    "    optimizer=optimizer, \n",
    "    net=net,\n",
    "    num_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test set DataLoader\n",
    "dataloader_test = DataLoader(\n",
    "    test_data, \n",
    "    batch_size=10,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics\n",
    "accuracy_metric = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "precision_metric = Precision(task='multiclass', num_classes=num_classes, average=None)\n",
    "recall_metric = Recall(task='multiclass', num_classes=num_classes, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "predictions = []\n",
    "for i, (features, labels) in enumerate(dataloader_test):\n",
    "    output = net.forward(features.reshape(-1, 1, image_size, image_size))\n",
    "    cat = torch.argmax(output, dim=-1)\n",
    "    predictions.extend(cat.tolist())\n",
    "    accuracy_metric(cat, labels)\n",
    "    precision_metric(cat, labels)\n",
    "    recall_metric(cat, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9003833532333374\n",
      "Precision (per class):  [0.8476049900054932, 0.9664167165756226, 0.8689544796943665, 0.9067513346672058, 0.8268320560455322, 0.9869996309280396, 0.7153656482696533, 0.93421471118927, 0.9760268330574036, 0.9791844487190247]\n",
      "Recall (per class):  [0.8611666560173035, 0.9879999756813049, 0.8144999742507935, 0.9043333530426025, 0.8443333506584167, 0.9616666436195374, 0.7254999876022339, 0.9775000214576721, 0.9703333377838135, 0.9564999938011169]\n"
     ]
    }
   ],
   "source": [
    "# Compute the metrics\n",
    "accuracy = accuracy_metric.compute().item()\n",
    "precision = precision_metric.compute().tolist()\n",
    "recall = recall_metric.compute().tolist()\n",
    "print('Accuracy: ', accuracy)\n",
    "print('Precision (per class): ', precision)\n",
    "print('Recall (per class): ', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
